{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 6 Exercise 2: finding the best parameters for predicting the fare of taxi rides\n",
    "We return to our Random Forest Regression and want to automatically optimize all free parameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the data we have saved after wrangling and pre-processing in block I\n",
    "X=pd.read_csv('../../DATA/train_cleaned.csv')\n",
    "drop_columns=['Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','key','pickup_datetime','pickup_date','pickup_latitude_round3','pickup_longitude_round3','dropoff_latitude_round3','dropoff_longitude_round3']\n",
    "X=X.drop(drop_columns,axis=1)\n",
    "X=pd.get_dummies(X)# one hot coding\n",
    "#generate labels\n",
    "y=X['fare_amount']\n",
    "X=X.drop(['fare_amount'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Optimize\n",
    "Scikit Optimize (https://scikit-optimize.github.io/stable/index.html) is a AutoML toolbox wrapped around Scikit-Learn. It allows us to use state-of-the-art automatic hyper-parameter optimization on top of our learning algorithms.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.7.4-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /home/keuper/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /home/keuper/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/keuper/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/keuper/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (1.18.1)\n",
      "Collecting pyaml>=16.9\n",
      "  Downloading pyaml-20.4.0-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: PyYAML in /home/keuper/anaconda3/lib/python3.7/site-packages (from pyaml>=16.9->scikit-optimize) (5.3)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-20.4.0 scikit-optimize-0.7.4\n"
     ]
    }
   ],
   "source": [
    "# install \n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E 2.1 Bayesian Optimization of a Random Forest Regression Model\n",
    "use Bayesian Optimization with Cross-Validation (https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV) to find the best regression model. Compare\n",
    "* linear regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) \n",
    "* Random Forest regression (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "* and SVM regression (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)\n",
    "\n",
    "NOTES: this can become quite compute intensive! Hence,\n",
    "* use a smaller subset of the training data to run the experiments \n",
    "* think about the range of your parameters (e.g. larger number of trees in RF or high C-values in SMV will make models expensive)\n",
    "* optimize only the following parameters per model type:\n",
    "    * linear: no parameters to optimize\n",
    "    * RF: #trees and depth\n",
    "    * SVM: C and gamma (use RBF kernel)\n",
    "* parallelize -> n_jobs\n",
    "* use CoLab to rum the job for up to 12h \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.to_numpy()[:10000,:], y.to_numpy()[:10000]) #subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fisrt, simple example using only RF\n",
    "opt = BayesSearchCV( \n",
    "         RandomForestRegressor(),\n",
    "         {\n",
    "             'n_estimators':Integer(10,200, prior='log-uniform'),\n",
    "             'max_depth':Integer(2,20,prior='log-uniform')\n",
    "         },\n",
    "         n_iter=32,\n",
    "         random_state=0,\n",
    "         n_jobs=4, #parallelize\n",
    "         cv=5, # set to 5 folds\n",
    "         scoring='neg_mean_squared_error' #eval MSE as before -> see https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter for other scoring methods\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5, error_score='raise',\n",
       "              estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              criterion='mse', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False...\n",
       "              fit_params=None, iid=True, n_iter=32, n_jobs=4, n_points=1,\n",
       "              optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=0,\n",
       "              refit=True, return_train_score=False,\n",
       "              scoring='neg_mean_squared_error',\n",
       "              search_spaces={'max_depth': Integer(low=2, high=20, prior='log-uniform', transform='identity'),\n",
       "                             'n_estimators': Integer(low=10, high=200, prior='log-uniform', transform='identity')},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-22.211899628933683"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('max_depth', 14), ('n_estimators', 127)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, more complicated example with several model types\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# pipeline class is used as estimator to enable\n",
    "# search over different model types\n",
    "pipe = Pipeline([\n",
    "    ('model', SVR()) #just put one model as placeholder\n",
    "])\n",
    "\n",
    "# single categorical value of 'model' parameter is\n",
    "# sets the model class\n",
    "# We will get ConvergenceWarnings because the problem is not well-conditioned.\n",
    "# But that's fine, this is just an example.\n",
    "rf_search = {\n",
    "    'model': Categorical([RandomForestRegressor()]),\n",
    "    'model__n_estimators': Integer(10, 100, 'log-uniform'),\n",
    "    \n",
    "}\n",
    "\n",
    "# explicit dimension classes can be specified like this\n",
    "svr_search = {\n",
    "    'model': Categorical([SVR()]),\n",
    "    'model__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "    'model__gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "}\n",
    "\n",
    "lin_search = {\n",
    "    'model' : Categorical([LinearRegression()]),\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    pipe,\n",
    "    [(svr_search, 10), (rf_search, 10), (lin_search,1)], # (parameter space, # of evaluations)\n",
    "    cv=5,\n",
    "     n_iter=32,\n",
    "    random_state=0,\n",
    "    n_jobs=4, #parallelize\n",
    "    scoring='neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val. score: -20.956843783427853\n",
      "test score: -25.538474243667295\n"
     ]
    }
   ],
   "source": [
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model',\n",
       "              RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                                    max_samples=None, min_impurity_decrease=0.0,\n",
       "                                    min_impurity_split=None, min_samples_leaf=1,\n",
       "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                                    n_estimators=33, n_jobs=None, oob_score=False,\n",
       "                                    random_state=None, verbose=0, warm_start=False)),\n",
       "             ('model__n_estimators', 33)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
